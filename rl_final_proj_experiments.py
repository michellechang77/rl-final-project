# -*- coding: utf-8 -*-
"""RL_Final_Proj_Experiments.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12pyLzKVljD3Sd6YfqWNZlaBp_ZEQw9bl
"""


# from google.colab import drive
# drive.mount('/content/drive')

# # Commented out IPython magic to ensure Python compatibility.
# # %pip install git+https://github.com/metadriverse/metadrive.git
# !pip install stable-baselines3  # For PPO experiments
# !pip install gym  # Ensure gym is installed

# !apt-get install -y xvfb python-opengl ffmpeg

# !pip install pyvirtualdisplay

# !sudo apt-get update
# !sudo apt-get install -y xvfb

import os
import numpy as np
from stable_baselines3 import PPO
from metadrive.envs import MetaDriveEnv
from PIL import Image
from IPython.display import display as ipy_display, clear_output
from stable_baselines3.common.monitor import Monitor
import matplotlib.pyplot as plt
from stable_baselines3.common.callbacks import BaseCallback
from datetime import datetime
import torch
import pandas as pd
from google.cloud import storage
from IPython import display

from stable_baselines3.common.callbacks import CheckpointCallback
from stable_baselines3.common.callbacks import EvalCallback


class PerformanceLogger(BaseCallback):
    def __init__(self, verbose=0):
        super(PerformanceLogger, self).__init__(verbose)
        self.timesteps = []
        self.rewards = []
        self.current_reward = 0
        self.reward_dict = {}
        self.start_time = datetime.now()
        self.clock_duration = []

    def reset(self):
        """Reset the logger for a new training phase."""
        self.current_reward = 0

    def _on_step(self) -> bool:
        # Sum rewards from this step
        self.current_reward += sum(self.locals["rewards"])

        # Log rewards at the end of a rollout
        if self.n_calls % self.model.n_steps == 0:
            self.timesteps.append(self.num_timesteps)
            self.rewards.append(
                self.current_reward / self.model.n_steps
            )  # Average reward per step
            self.current_reward = 0  # Reset for next rollout
            self.clock_duration.append(datetime.now() - self.start_time)
            # self.reward_dict[self.num_timesteps] = self.rewards[-1]
        return True


# Function to create the environment
def create_environment(difficulty, render=False, monitor=True):
    configs = {
        "easy": {
            "map": 2,
            "traffic_density": 0.0,
            "random_lane_width": False,
            "use_render": render,
            #  "out_of_road_penalty": 30.0,  # Increased penalty
            #  "out_of_road_cost":
        },
        "medium": {
            "map": 3,
            "traffic_density": 0.3,
            "random_lane_width": True,
            "use_render": render,
            #  "out_of_road_penalty": 30.0,  # Increased penalty
        },
        "hard": {
            "map": 4,
            "traffic_density": 0.5,
            "random_lane_width": True,
            "use_render": render,
        },
        "eval_easy": {
            "map": 5,
            "traffic_density": 0.0,
            "random_lane_width": False,
            "use_render": render,
            #  "out_of_road_penalty": 30.0,  # Increased penalty
            #  "out_of_road_cost":
        },
        "eval_medium": {
            "map": 5,
            "traffic_density": 0.3,
            "random_lane_width": True,
            "use_render": render,
            #  "out_of_road_penalty": 30.0,  # Increased penalty
        },
        "eval_hard": {
            "map": 5,
            "traffic_density": 0.5,
            "random_lane_width": True,
            "use_render": render,
        },
    }

    base_env = MetaDriveEnv(configs[difficulty])
    if monitor:
        return Monitor(base_env)
    else:
        return base_env


def train_agent_with_transfer(
    env, timesteps=1000, model=None, callbacks=None, checkpoint_dir="./"
):
    if model is None:
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"Using device: {device}")
        model = PPO(
            "MlpPolicy", env, verbose=2, n_steps=4096, device=device
        )  # changing n_step param significantly changes reward results, should play ard with this
    else:
        print("Continuing training with transfer learning...")
    logger = PerformanceLogger()
    # model.learn(total_timesteps=100000, callback=checkpoint_callback)
    checkpoint_callback = CheckpointCallback(
        save_freq=20_000, save_path=checkpoint_dir, name_prefix="ppo_model"
    )
    logger.reset()
    # logger.reset()  # Reset logger for the new training phase
    model.learn(total_timesteps=timesteps, callback=[logger, checkpoint_callback])
    print("loggerrewards", logger.rewards)
    return model, logger


def plot_performance(logger, title="Agent Performance", save_path=None):
    """
    Plot the logged performance data.
    """
    if not logger.timesteps or not logger.rewards:
        print("[ERROR] No data to plot. Ensure rewards are logged during training.")
        return

    plt.figure(figsize=(12, 6))
    plt.plot(logger.timesteps, logger.rewards, label="Episode Rewards")
    plt.xlabel("Timesteps")
    plt.ylabel("Rewards")
    plt.title("Agent Performance Over Timesteps")
    plt.legend()
    plt.grid()

    # if save_path:
    plt.savefig(os.path.join(save_path, "training_rewards.png"))
    plt.show()


def init_dirs(trial_name, difficulty):
    trial_dir = os.path.join("trials", trial_name)
    log_dir = os.path.join(trial_name, difficulty, "logs")
    checkpoint_dir = os.path.join(trial_dir, difficulty, "checkpoints")
    best_model_dir = os.path.join(trial_dir, difficulty, "best_model")
    gif_dir = os.path.join(trial_dir, difficulty, "gifs")
    os.makedirs(trial_dir, exist_ok=True)
    os.makedirs(log_dir, exist_ok=True)
    os.makedirs(checkpoint_dir, exist_ok=True)
    os.makedirs(best_model_dir, exist_ok=True)
    os.makedirs(gif_dir, exist_ok=True)
    return trial_dir, log_dir, checkpoint_dir, best_model_dir, gif_dir


def get_callbacks(checkpoint_dir, best_model_dir, log_dir, eval_env):
    logger = PerformanceLogger()
    # model.learn(total_timesteps=100000, callback=checkpoint_callback)
    checkpoint_callback = CheckpointCallback(
        save_freq=10_000, save_path=checkpoint_dir, name_prefix="ppo_model"
    )
    # TODO: get filepaths right
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=best_model_dir,
        log_path=log_dir,
        eval_freq=1000,  # Evaluate every 10,000 steps
        n_eval_episodes=5,  # Number of episodes per evaluation
        deterministic=True,
        render=False,
    )
    return eval_callback, checkpoint_callback, logger


def upload_folder_to_gcs(local_folder, bucket_name, gcs_prefix):
    """
    Uploads the contents of a local folder (including subdirectories) to a GCS bucket,
    preserving the folder structure under a given prefix.

    Args:
        local_folder (str): Path to the local folder.
        bucket_name (str): Name of the GCS bucket.
        gcs_prefix (str): Prefix for the uploaded files in the bucket.

    Returns:
        None
    """
    # Initialize the GCS client
    client = storage.Client()
    bucket = client.bucket(bucket_name)

    # Walk through the local folder
    for root, dirs, files in os.walk(local_folder):
        for file_name in files:
            # Get the full local path
            local_path = os.path.join(root, file_name)

            # Construct the relative path in GCS
            relative_path = os.path.relpath(local_path, local_folder)
            gcs_path = os.path.join(gcs_prefix, relative_path).replace(
                "\\", "/"
            )  # Ensure GCS path is POSIX-style

            # Upload the file
            blob = bucket.blob(gcs_path)
            blob.upload_from_filename(local_path)
            print(f"Uploaded {local_path} to gs://{bucket_name}/{gcs_path}")


def create_gif(env, model, gif_dir):
    obs, _ = env.reset()
    for i in range(1_000):
        action, _states = model.predict(obs, deterministic=True)
        obs, reward, done, _, info = env.step(action)
        # total_reward += reward
        if isinstance(env, Monitor):
            e = env.env
        else:
            e = env
        ret = e.render(
            mode="topdown",
            screen_record=True,
            window=False,
            screen_size=(600, 600),
            camera_position=(50, 50),
        )
        if done:
            # print("episode_reward", total_reward)
            break

    env.top_down_renderer.generate_gif(os.path.join(gif_dir, "demo.gif"))
    img = display.Image(filename=os.path.join(gif_dir, "demo.gif"))
    # display(img)
    return img


def run_trial(trial_name, difficulty, model, transfer, timesteps_per_difficulty):
    trial_dir, log_dir, checkpoint_dir, best_model_dir, gif_dir = init_dirs(
        trial_name, difficulty
    )
    eval_env = create_environment(difficulty, render=False, monitor=False)

    print(f"Training on {difficulty} difficulty (With Transfer={transfer})...")
    env = create_environment(difficulty, render=False)
    try:
        eval_callback, checkpoint_callback, _ = get_callbacks(
            checkpoint_dir, best_model_dir, log_dir, eval_env
        )
        # try:
        #   x=evaluate_model(model, eval_env, num_episodes=10, max_timesteps_per_episode=1000)
        #   rews = pd.DataFrame(x)
        #   pd.to_csv(os.path.join(trial_dir, 'test_rewards_pre_training.csv'), index=False)
        # except Exception as e:
        #   print(f"Error during gif creation/test eval with ", e)
        model, logger = train_agent_with_transfer(
            env,
            model=model,
            timesteps=timesteps_per_difficulty,
            callbacks=[checkpoint_callback],  # eval_callback,
            checkpoint_dir=checkpoint_dir,
        )
        df = pd.DataFrame([logger.timesteps, logger.rewards, logger.clock_duration])
        df = df.T
        df.columns = ["timesteps", "rewards", "duration"]
        df.to_csv(os.path.join(best_model_dir, "training_rewards.csv"))
        # model = PPO.load(os.path.join(best_model_dir, 'best_model'))
    except Exception as e:
        print(f"Error during training on {difficulty} difficulty: {e}")
    finally:
        env.close()
        try:
            x = evaluate_model(
                model, eval_env, num_episodes=10, max_timesteps_per_episode=1000
            )
            rews = pd.DataFrame(x)
            rews.to_csv(
                os.path.join(best_model_dir, "test_rewards_post_training.csv"),
                index=False,
            )
            create_gif(eval_env, model, gif_dir)
            model.save(best_model_dir)
        except Exception as e:
            print(f"Error during gif creation/test eval with ", e)
        eval_env.close()
    # print("after trail rewards ", logger.rewards)
    # Plot performance for each difficulty
    # print(f"Performance for {difficulty} difficulty:"
    return logger, model, None


# Curriculum experiment with transfer learning
def curriculum_experiment(
    difficulty_order,
    timesteps_per_difficulty=1000,
    transfer=True,
    trial_name=None,
    model=None,
):
    if trial_name is None:
        trial_name = datetime.now().strftime("%Y%m%d-%H%M%S")
    results = []
    models = []
    final_models = []

    for difficulty in difficulty_order:
        trial_dir, log_dir, checkpoint_dir, best_model_dir, gif_dir = init_dirs(
            trial_name, difficulty
        )
        result, model, final_model = run_trial(
            trial_name, difficulty, model, transfer, timesteps_per_difficulty
        )
        results.append((difficulty, result))
        models.append((difficulty, model))
        final_models.append((difficulty, final_model))
        if transfer:
            model = final_model
        else:
            model = None
        print(f"Performance for {difficulty} difficulty:")
        plot_performance(
            result,
            f"Performance on {difficulty} (With Transfer={transfer})",
            best_model_dir,
        )
    # Plot performance for each difficulty
    # for difficulty, logger in results:

    return results, models, final_models


def evaluate_model(model, env, num_episodes=10, max_timesteps_per_episode=1000):
    """
    Evaluate a trained model in the given environment.

    Parameters:
    - model: The trained RL model to evaluate.
    - env: The environment to evaluate the model in.
    - num_episodes: Number of episodes to run for evaluation.
    - max_timesteps_per_episode: Maximum timesteps per episode.

    Returns:
    - results: A list of dictionaries containing total reward and termination reason for each episode.
    """
    results = []

    for episode in range(num_episodes):
        obs, _ = env.reset()
        total_reward = 0
        termination_reason = None
        for i in range(1_000):
            action, _states = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            total_reward += reward
            # total_reward += reward
            if isinstance(env, Monitor):
                e = env.env
            else:
                e = env
            ret = e.render(
                mode="topdown",
                screen_record=True,
                window=False,
                screen_size=(600, 600),
                camera_position=(50, 50),
            )

            if terminated or truncated:
                if info.get("arrive_dest", False):
                    termination_reason = "Destination Reached"
                elif info.get("crash", False):
                    termination_reason = "Collision"
                elif info.get("out_of_road", False):
                    termination_reason = "Off Road"
                else:
                    termination_reason = "Other"
                break

        results.append(
            {
                "episode": episode + 1,
                "total_reward": total_reward,
                "termination_reason": termination_reason,
            }
        )

    return results


def main(time_steps, difficulty_order, transfer, trial_name=None):

    results, models, final_models = curriculum_experiment(
        difficulty_order,
        timesteps_per_difficulty=time_steps,
        transfer=transfer,
        trial_name=trial_name,
    )

    upload_folder_to_gcs("trials", "snap-chef-recipe", "rl-final-project")


if __name__ == "__main__":
    time_steps = 200
    difficulty_order = ["easy", "medium", "hard"]
    main(time_steps, difficulty_order)
